suspense.tag.values<-NULL
color.tags<-c("highSuspense1", "highSuspense2", "highSuspense3", "highSuspense4", "lowSuspense1", "lowSuspense2",  "lowSuspense3", "lowSuspense4")
number.sequence<-c(5,6,7,8,1,2,3,4)
color.table<-cbind(color.tags, number.sequence)
for(i in 1:nrow(meta.table)){
curr.id<-meta.table[i,idCol]
curr.id<-unlist(strsplit(curr.id, '.txt'))
if(curr.id %in% file.names){
to.open<-paste(curr.id, "_autotagged.txt", sep='')
curr.file<-scan(paste(tag.text.dir, to.open, sep='/'), what='character', encoding='UTF-8', sep='\n')
curr.file<-paste(curr.file, collapse=' ')
curr.file<-unlist(strsplit(curr.file, ' '))
text.length<-length(curr.file)
text.info<-plotTags(curr.file, color.table, plot.type = 'line')
curr.totalsuspense<-text.info$TotalSuspense
curr.line.length<-dist.cal(1,text.info$SuspenseValues)
curr.changes<-changes.cal(text.info$SuspenseValues)
all.info<-c(curr.totalsuspense, curr.line.length, curr.changes, text.length)
table.add<-rbind(table.add, all.info)
suspense.tag.values.line<-rep(NA, 102)
suspense.tag.values.line[1:length(text.info$SuspenseValues)]<-c(meta.table$Title[i], text.info$SuspenseValues)
suspense.tag.values<-rbind(suspense.tag.values, suspense.tag.values.line)
} else {
table.add<-rbind(table.add, rep(NA, 3))
}
}
table.add<-as.data.frame(table.add, stringsAsFactors=F)
colnames(table.add)<-c("Total_Suspense", "Suspense_Line_Length", "Number_of_Line_Changes", "Text_Length")
new.meta.table<-cbind(meta.table, table.add)
return.list<-list(new.meta.table, suspense.tag.values)
names(return.list)<-c("FullTable", "SuspenseValues")
return(return.list)
}
#creates a feature table suitable for training a Neural Net. Identifies field percentages and tags as a
#primary function but, if add.metrics is set to T, includes narrative percentage and POS percentages as
#additional features.
neuralNetTable<-function(source.text.dir,
tag.text.dir,
convert.source.dir,
convert.tag.dir,
output.dir,
fields,
tags,
window.type='percentage',
window.advance=1,
window=2,
type='text',
add.metrics=T,
output.file="NeuralNet/New Revised Model/NewMasterFeatureTable.csv"){
complete.feature.table<-data.frame()
curr.output<-multi.field.plot(convert.source.dir,
convert.tag.dir,
output.dir,
fields,
tags,
window.type=window.type,
window.advance=window.advance,
window=window,
type=type,
poly.degree='line',
convert.tags=T,
add.metrics=add.metrics,
output='table')
complete.feature.table<-rbind(complete.feature.table, curr.output)
curr.output<-multi.field.plot(source.text.dir,
tag.text.dir,
output.dir,
fields,
tags,
window.type=window.type,
window.advance=window.advance,
window=window,
type=type,
poly.degree='line',
convert.tags=F,
add.metrics=add.metrics,
output='table')
complete.feature.table<-rbind(complete.feature.table, curr.output)
write.csv(complete.feature.table, file=output.file)
return(complete.feature.table)
}
#calculate the percent narrative completion per slice
calNarrativePerc<-function(centroid.vec, text.length){
text.percents<-(centroid.vec/text.length)*100
text.percents[which(text.percents<26)]<-1
text.percents[which(text.percents>74)]<-3
text.percents[which(!text.percents %in% c(1,3))]<-2
return(text.percents)
}
#find a set of pos scores in a given slice
posCal<-function(pos.slice){
slice.length<-length(pos.slice)
slice.cut<-unlist(strsplit(pos.slice, "_"))
#raw_verbs<-which(pos.slice %in% (c("VB", "VBD", "VBG", "VBN", "VBP", "VBZ")))
#raw_verbs<-length(raw_verbs)/slice.length
#raw_nouns<-which(pos.slice %in% c("NN", "NNS", "NNP", "NNPS"))
#raw_nouns<-length(raw_nouns)/slice.length
#raw_adj<-which(pos.slice %in% c("JJ", "JJR", "JJS"))
#raw_adj<-length(raw_adj)/slice.length
#raw_adv<-which(pos.slice %in% c("RB", "RBR", "RBS"))
#raw_adv<-length(raw_adv)/slice.length
#slice.pos<-c(raw_verbs, raw_nouns, raw_adj, raw_adv)
pos.ids<-c("CC", "CD", "DT", "EX", "FW", "IN", "JJ", "JJR", "JJS", "MD", "NN", "NNS", "NNP", "NNPS", "PDT", "PRP", "PRP$", "RB", "RBR", "RBS", "RP", "TO", "UH", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ", "WDT", "WP", "WP$", "WRB", ".", ",", ":")
pos.freq<-lapply(pos.ids, function(x) (length(which(slice.cut==x))/length(pos.slice)))
pos.freq<-unlist(pos.freq)
return(pos.freq)
}
suspenseTagResolve<-function(feature.table, tag.col, suspense.tags=c("suspense", "suspense8", "suspense9", "suspense10"), unsuspense.tags=c("unsuspense", "nonsuspense", "suspense1", "suspense2", "suspense3")){
feature.table[which(feature.table[,tag.col] %in% suspense.tags),tag.col]<-"suspense"
feature.table[which(feature.table[,tag.col] %in% unsuspense.tags),tag.col]<-"unsuspense"
table.class<-feature.table[which(feature.table[,tag.col] %in% c("suspense", "unsuspense")),]
return(table.class)
}
testWindows<-function(text.dir, window.size, window.advance, type="percentage"){
test.files<-list.files(text.dir, pattern=".txt")
info.table<-list()
for(i in 1:length(test.files)){
infile<-paste(text.dir, test.files[i], sep='/')
curr.text<-scan(infile, what='character', sep="\n")
curr.text<-paste(curr.text, collapse=" ")
curr.text<-unlist(strsplit(curr.text, " "))
#window.s<-round((length(curr.text) * (window.size/100)),0)
#window.a<-round((length(curr.text) * (window.advance/100)), 0)
curr.split.table<-createWindows(curr.text, window.size, window.advance)
return.list<-c(length(curr.text), window.s, window.a, nrow(curr.split.table), curr.split.table[nrow(curr.split.table),2], curr.split.table[nrow(curr.split.table),3], (curr.split.table[nrow(curr.split.table),3]-curr.split.table[nrow(curr.split.table),1]))
info.table<-c(info.table, list(return.list))
}
info.matrix<-matrix(unlist(info.table), ncol=7, byrow=T)
info.matrix<-as.data.frame(info.matrix, stringsAsFactors=F)
colnames(info.matrix)<-c("TextLength", "WindowSize", "WindowAdvance", "NumDivisions", "FinalCentroid", "FinalSliceEnd", "FinalSliceSize")
info.matrix$TextNames<-test.files
return(info.matrix)
}
#function takes a directory of texts (either pos or not) and uses a model to tag all texts and output them to a new directory
BrickTag<-function(class.model,
class.fields,
plot.type="line",
plot.grid=T,
indir="OriginalTexts",
outdir.plot="Plots",
outdir.text="TaggedTexts",
div.size=2,
div.advance=1,
div.type='percentage',
aoa=F,
pos=F,
add.metrics=T,
output.stats=T,
smooth.plot=F){
file.list<-list.files(indir, pattern='.txt')
print(file.list)
#sorts files into dates a-ssuming date is the last four characters
file.list<-sortFiles(file.list)
file.list<-paste(indir, file.list, sep='/')
text.list<-lapply(file.list, function(x) scan(x, what='character', sep='\n', encoding='UTF-8'))
if (pos){
text.list<-lapply(text.list, function(x) paste(x, collapse=' '))
text.list<-lapply(text.list, function(x) unlist(strsplit(x, ' ')))
text.list<-lapply(text.list, function(x) hardPOSClean(x))
} else {
text.list<-lapply(text.list, function(x) paste(x, collapse=' '))
}
print("Calculating POS values")
source(paste(dropbox.path, "POS.R", sep='/'))
pos.text.list<-lapply(text.list, function(x) pos_tag_file(x))
pos.text.list<-lapply(pos.text.list, function(x) unlist(strsplit(x, ' ')))
remove(text.list)
tagged.texts<-lapply(pos.text.list, function(x) autoTag(x, div.size=div.size, div.advance=div.advance, div.type=div.type, class.model=class.model, class.fields=class.fields, aoa=aoa, pos=pos, plot.type=plot.type, add.metrics=add.metrics, smooth.plot=smooth.plot))
file.list<-as.list(file.list)
raw.text.names<-lapply(file.list, function(x) unlist(strsplit(x, '.txt')))
raw.text.names<-lapply(raw.text.names, function(x) unlist(strsplit(x, '/')))
#print(raw.text.names)
#print(length(raw.text.names))
raw.text.names<-lapply(raw.text.names, function(x) x[length(raw.text.names[[1]])])
text.names<-paste(raw.text.names, '_autotagged.txt', sep='')
plot.names<-paste(raw.text.names, '_autotagged_plots.pdf', sep='')
text.names<-paste(outdir.text, text.names, sep='/')
plot.names<-paste(outdir.plot, plot.names, sep='/')
tagged.text.words<-lapply(tagged.texts, function(x) x[[2]])
tagged.text.plots<-lapply(tagged.texts, function(x) x[[1]])
if(output.stats){
if(plot.type=="line"){
tagged.text.stats<-lapply(tagged.texts, function(x) x[[5]])
suspense.tags<-lapply(tagged.texts, function(x) x[[4]][,1])
suspense.tags<-unlist(suspense.tags)
full.matrix<-do.call(rbind, tagged.text.stats)
full.matrix<-cbind(full.matrix, suspense.tags)
suspense.slice.scores<-lapply(tagged.texts, function(x) x[[6]])
suspense.slice.scores<-do.call(rbind, suspense.slice.scores)
rownames(suspense.slice.scores)<-unlist(raw.text.names)
write.csv(full.matrix, file=paste(outdir.plot, "AllSliceStats.csv", sep="/"))
write.csv(suspense.slice.scores, file=paste(outdir.plot, "SuspenseSliceScores.csv", sep="/"))
} else {
print("Stats Unavailable in Bar Graphs")
}
}
if(plot.type=="line"){
total.suspense<-lapply(tagged.texts, function(x) x[[3]])
total.suspense<-unlist(total.suspense)
all.dates<-lapply(file.list, function(x) extractDate(x))
all.dates<-unlist(all.dates)
total.suspense.table<-data.frame(unlist(raw.text.names), all.dates, total.suspense, stringsAsFactors=F)
write.csv(total.suspense.table, file=paste(outdir.plot, "totalsuspense_Unsuspensecorpus.csv", sep="/"))
}
mapply(function(x,y) write(x,file=y), tagged.text.words, text.names)
#print each plot to its own file
mapply(function(x,y) plotPDF(x,y), plot.names, tagged.text.plots)
if(plot.grid){
side=sqrt(length(tagged.text.plots))
hor=round(side, 0)
vert=round(side, 0)
vert=vert+1
pdf(paste(outdir.plot, "AllPlots.pdf", sep='/'), height=vert*3, width=hor*8)
library(gridExtra)
all.plots<-marrangeGrob(tagged.text.plots, ncol=hor, nrow=vert)
print(all.plots)
#ggsave(paste(outdir.plot, "allplots_test.pdf", sep='/'), all.plots)
#lapply(tagged.text.plots, function(x) print(x))
dev.off()
}
names(tagged.texts)<-text.names
#return(tagged.texts)
detach(package:ggplot2, unload=T)
}
#function takes a directory of texts (either pos or not) and uses a model to tag all texts and output them to a new directory
BrickTagBigCorpus<-function(class.model,
class.fields,
plot.type="line",
indir="OriginalTexts",
outdir.plot="Plots",
outdir.text="TaggedTexts",
div.size=2,
div.advance=1,
div.type='percentage',
aoa=F,
pos=F,
add.metrics=T,
output.stats=T,
smooth.plot=T){
all.file.list<-list.files(indir, pattern='.txt')
#print(all.file.list)
#sorts files into dates assuming date is the last four characters
#all.file.list<-sortFiles(file.list)
all.file.list.dir<-paste(indir, all.file.list, sep='/')
all.stats<-NULL
for(i in 1:length(all.file.list.dir)){
file.list<-all.file.list.dir[i]
filename<-all.file.list[i]
print(file.list)
text.list<-lapply(file.list, function(x) scan(x, what='character', sep='\n', encoding='UTF-8'))
if (pos){
text.list<-lapply(text.list, function(x) paste(x, collapse=' '))
text.list<-lapply(text.list, function(x) unlist(strsplit(x, ' ')))
text.list<-lapply(text.list, function(x) hardPOSClean(x))
} else {
text.list<-lapply(text.list, function(x) paste(x, collapse=' '))
}
print("Calculating POS values")
source(paste(dropbox.path, "POS.R", sep='/'))
pos.text.list<-lapply(text.list, function(x) pos_tag_file(x))
pos.text.list<-lapply(pos.text.list, function(x) unlist(strsplit(x, ' ')))
remove(text.list)
tagged.texts<-lapply(pos.text.list, function(x) autoTag(x, div.size=div.size, div.advance=div.advance, div.type=div.type, class.model=class.model, class.fields=class.fields, aoa=aoa, pos=pos, plot.type=plot.type, add.metrics=add.metrics, smooth.plot=smooth.plot))
file.list<-as.list(file.list)
raw.text.names<-lapply(file.list, function(x) unlist(strsplit(x, '.txt')))
raw.text.names<-lapply(raw.text.names, function(x) unlist(strsplit(x, '/')))
#print(raw.text.names)
#print(length(raw.text.names))
raw.text.names<-lapply(raw.text.names, function(x) x[length(raw.text.names[[1]])])
text.names<-paste(raw.text.names, '_autotagged.txt', sep='')
plot.names<-paste(raw.text.names, '_autotagged_plots.pdf', sep='')
text.names<-paste(outdir.text, text.names, sep='/')
plot.names<-paste(outdir.plot, plot.names, sep='/')
tagged.text.words<-lapply(tagged.texts, function(x) x[[2]])
tagged.text.plots<-lapply(tagged.texts, function(x) x[[1]])
if(output.stats){
if(plot.type=="line"){
suspense.tags<-lapply(tagged.texts, function(x) x[[4]][,1])
suspense.tags<-as.numeric(unlist(suspense.tags))
if(length(suspense.tags)<105){
to.fill<-105-length(suspense.tags)
suspense.tags<-c(suspense.tags, rep(NA,to.fill))
}
all.stats<-rbind(all.stats, c(filename, suspense.tags))
} else {
#print("Stats Unavailable in Bar Graphs")
}
}
if(plot.type=="line"){
#total.suspense<-lapply(tagged.texts, function(x) x[[3]])
#total.suspense<-unlist(total.suspense)
#all.dates<-lapply(file.list, function(x) extractDate(x))
#all.dates<-unlist(all.dates)
#total.suspense.table<-data.frame(unlist(raw.text.names), all.dates, total.suspense, stringsAsFactors=F)
#write.csv(total.suspense.table, file=paste(outdir.plot, "totalsuspense_Unsuspensecorpus.csv", sep="/"))
}
mapply(function(x,y) write(x,file=y), tagged.text.words, text.names)
#print each plot to its own file
mapply(function(x,y) plotPDF(x,y), plot.names, tagged.text.plots)
names(tagged.texts)<-text.names
#return(tagged.texts)
detach(package:ggplot2, unload=T)
write.csv(all.stats, file=paste(outdir.plot, "AllStats.csv", sep="/"), row.names=F)
}
}
pwd
ls
ls()
getwd()
setwd("~/Projects/suspense/Brick")
getwd()
load("Brick.RData")
net.table
suspense.fields
net.table[1]
Brick
Brick[1]
Brick[[1]]
Brick$net.model
autoTag("this is a novel", 25, 25, class.model=Brick$net.model, class.fields=suspense.fields, plot.type = "line")
autoTag("this is a novel more words very suspenseful", 25, 25, class.model=Brick$net.model, class.fields=suspense.fields, plot.type = "line")
autoTag("this is a novel more words very suspenseful", 1, 1, class.model=Brick$net.model, class.fields=suspense.fields, plot.type = "line")
soft_clean<-function(text){
text.split<-unlist(strsplit(text, ''))
print(length(text.split))
#keep.char<-c(letters, LETTERS, '.', "?", "!", ",",";",":","'","\"","(",")","[","]","<",">", "&", ' ', '-', '\n')
keep.char<-c(letters, LETTERS, '.', "?", "!", ",",";",":", "&", " ", "\n")
keep.index<-which(text.split %in% keep.char)
#print(length(keep.index))
num.index<-which(!is.na(as.numeric(text.split)))
full.index<-c(keep.index)
text.clean<-text.split[full.index]
text.clean<-paste(text.clean, collapse='')
return(text.clean)
}
sentence_split<-function(text, separate="_", no.caps=T, max.sent=100){
library (openNLP)
library (openNLPdata)
library (NLP)
#define annotator based on Maxent OpenNLP package (sentence, word and part of speech)
sent_annotate<-Maxent_Sent_Token_Annotator()
#turn text into string
text<-as.String(text)
#divide text into sentences
text.sent<-annotate(text, list(sent_annotate))
#put sentences into vector of characters
sent.vector<-text[text.sent]
print(length(sent.vector))
#collapse sentences into blocks of text max.sent long
num.blocks<-length(sent.vector) %/% max.sent
block.vector<-NULL
start.pos<-1
end.pos<-max.sent
while (end.pos<=length(sent.vector)){
curr.block<-sent.vector[start.pos:end.pos]
block.char<-paste(curr.block, collapse=' ')
block.vector<-c(block.vector, block.char)
start.pos<-end.pos+1
end.pos<-end.pos+max.sent
}
if(start.pos<length(sent.vector)){
curr.block<-sent.vector[start.pos:length(sent.vector)]
block.char<-paste(curr.block, collapse=' ')
block.vector<-c(block.vector, block.char)
}
#remove unneeded varaibles to free space
remove(text)
remove(text.sent)
remove(sent.vector)
gc(verbose=FALSE)
#variable to put attached text into
full.text<-NULL
#print(length(sent.vector))
#loop through sentences and annotate with POS tagger
for (i in 1:length(block.vector)){
#print(i)
curr.block.tagged<-pos_tagger(block.vector[i], separate=separate, no.caps=no.caps)
gc(verbose=FALSE)
full.text<-c(full.text, curr.block.tagged)
}
full.text<-paste(full.text, collapse=' ')
return(full.text)
}
pos_tagger<-function(text, separate="_", no.caps=T){
require(openNLP)
require(openNLPdata)
require(NLP)
#define annotators based on Maxent OpenNLP package (sentence, word and part of speech)
sent_annotate<-Maxent_Sent_Token_Annotator()
word_annotate<-Maxent_Word_Token_Annotator()
pos_annotate<-Maxent_POS_Tag_Annotator()
#turn text into string
text.s<-as.String(text)
#create annotation variable
token_annotation<-annotate(text.s, list(sent_annotate, word_annotate))
pos_annotation<-annotate(text.s, pos_annotate, token_annotation)
#retrieve words from annotation object
pos_words<-subset(pos_annotation, type=="word")
#create POS tags
pos_tags<-sapply(pos_words$features, '[[', "POS")
#convert original text to lowercase
if(no.caps){
text.s<-tolower(text.s)
}
#create sprintf separator
sep.char<-paste("%s", separate, "%s", sep='')
#bind tags to original text with separator
tagged.string<-sprintf(sep.char, text.s[pos_words], pos_tags)
#collapse the list of tagged words into character string
tagged.text<-paste(tagged.string, collapse=" ")
return(tagged.text)
}
pos_tag_file<-function(text, separate="_", no.caps=T){
text<-unlist(strsplit(text, ''))
sep.char.index<-which(text==separate)
if (length(sep.char.index)>0){
text<-text[-sep.char.index]
}
text<-paste(text, collapse='')
text<-soft_clean(text)
#in.file<unlist(strsplit(in.file, ' '))
file.tagged<-sentence_split(text, separate, no.caps)
return(file.tagged)
}
pos_tag_dir<-function(in.dir, out.dir, separate="_", no.caps=T, resume=F){
file.list<-list.files(path=in.dir, pattern=".txt")
print(length(file.list))
if(resume){
outfile.list<-list.files(path=out.dir, pattern=".txt")
outfile.list.stem<-unlist(strsplit(outfile.list, '_pos.txt'))
file.list.stem<-unlist(strsplit(file.list, '.txt'))
done.index<-which(file.list.stem %in% outfile.list.stem)
file.list<-file.list[-done.index]
print(length(file.list))
}
bad.files<-NULL
for (i in 1:length(file.list)){
print(file.list[i])
file.path=paste(in.dir, file.list[i], sep="/")
in.file<-scan(file.path, what="character", sep='\n')
in.file<-paste(in.file, collapse=' ')
in.file<-unlist(strsplit(in.file, ''))
sep.char.index<-which(in.file==separate)
if (length(sep.char.index)>0){
in.file<-in.file[-sep.char.index]
}
in.file<-paste(in.file, collapse='')
in.file<-soft_clean(in.file)
#in.file<unlist(strsplit(in.file, ' '))
file.tagged<-try(sentence_split(in.file, separate, no.caps), silent=TRUE)
if ('try-error' %in% class(file.tagged)){
bad.files<-c(bad.files, file.list[i])
next
} else {
file.name<-unlist(strsplit(file.list[i], '.txt'))
new.file.name<-paste(file.name, "_pos.txt", sep='')
out.path<-paste(out.dir, new.file.name, sep="/")
write(file.tagged, out.path)
}
}
}
extractPOSTags<-function(pos.text, sep.tag="_", ret='POS', separate=F){
text.split<-unlist(strsplit(pos.text, ' '))
pos.split<-unlist(strsplit(text.split, sep.tag))
pos.index<-seq(2, length(pos.split), by=2)
text.index<-seq(1, length(pos.split), by=2)
if (ret=="POS"){
return.text<-pos.split[pos.index]
} else {
return.text<-pos.split[text.index]
}
if (separate==F){
return.text<-paste(return.text, collapse=' ')
}
return(return.text)
}
#extract either POS tags or raw words from a POS tagged corpus (ret can either be POS or Text)
posCorpus<-function(corpus, sep.tag="_", ret='POS'){
library(tm)
new.vector<-NULL
for (i in 1:length(corpus)){
extracted.text<-extractPOSTags(corpus[[i]], sep.tag, ret)
new.vector<-c(new.vector, extracted.text)
}
new.vectorsource<-VectorSource(new.vector)
extracted.corpus<-Corpus(new.vectorsource)
return(extracted.corpus)
}
writeAndConv<-function(text.vector, group.name){
groups<-rep(group.name, length(text.vector))
filenames<-paste(groups, "_", as.character(seq(1,length(groups), by=1)), ".txt", sep='')
for (i in 1:length(text.vector)){
converted.text<-iconv(text.vector[i], "UTF-8", "ASCII", sub='')
write(converted.text, file=filenames[i])
}
}
BrickTagBigCorpus(Brick$net.model, suspense.fields)
getwd()
source('~/Projects/suspense/Brick/Tagging_F.R')
BrickTagBigCorpus(Brick$net.model, suspense.fields)
install.packages("openNLP")
install.packages("gridExtra")
install.packages("neuralnet")
install.packages("tm")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
BrickTagBigCorpus(Brick$net.model, suspense.fields)
library(openNLP)
install.packages("openNLP")
install.packages("openNLPdata")
install.packages("rJava")
